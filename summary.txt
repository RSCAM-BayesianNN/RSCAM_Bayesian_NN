We started with a simple dataset and a shallow neural network to explore the possibilities. We used the Metropolis Hastings algorithm to sample from the posteriors of the weights. We plotted these posteriror along with the MLE estimates obtained from the gradient descent optimiser to check for overlap. Indeed, each MLE estimate landed very close to one of the posterior modes for that parameter. 

We then moved on to more complicated datasets (circle data). We tried increasing the number of layers in the network to adapt it to more complex problems. Once again, we trained the network and plotted the MLE estimates for the weights along with their posterior distributions obtained from MH sampling. The results are very promising. Not only is the sampler able to find all the MLE estimates, but as well as additional modes that might even reduce the loss further.
