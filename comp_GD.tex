\subsection{Comparison with gradient descent}
%maybe small explanation for SGD is warrented%

Fig shows that while most of optimised values of the parameters somewhat coincide with the mode of the posterior, especially in the uni-modal cases, the values of parameters associated with (blah blah) which have bi-modal distributions do not coincide with the mode that would give the lowest loss. We can see this in abundance in a more complex model running on a more complex dataset. 


\subsection{Gradient descent}
In order to compare the bayesian results to frequentist, we plotted the training loss for the gradient descent algorithm. We found that while it managed to provide excellent results for the linear classification data, the training loss did not converge to 0 when we replaced it with circle data.